1) "How do you monitor and troubleshoot AKS clusters? What tools do you use and what kinds of issues have you resolved?"

"To monitor and troubleshoot AKS clusters, we primarily use Azure Monitor, Application Insights, and kubectl commands.

When an issue occurs, we start by checking pod health, CPU/memory usage, and restarts using:

kubectl get pods
kubectl describe pod <pod-name>
kubectl top pod <pod-name>
kubectl logs <pod-name>


This helps us determine whether the issue is with the app, the container, or the cluster resource limits.

On the application side, we use Application Insights to track exceptions, response times, and failures. For example, the Transaction Search 
feature helps us trace failed requests and identify root causes from telemetry.

In one case, an API deployed to AKS was repeatedly restarting. Using kubectl logs, I identified out-of-memory errors, and with Azure Monitor, 
I noticed the pod was breaching its memory limit. We resolved it by increasing the pod memory in the Helm values file and redeploying."

1Ô∏è‚É£ How I monitor AKS day-to-day

Foundation setup

Azure Monitor + Container Insights

Enable diagnostics on the AKS cluster and send them to Log Analytics.

Turn on Container Insights for:

Node & pod CPU/memory

Node disk & filesystem

Controller status (deployments, DaemonSets, jobs)

Kube events

Application Insights

For HTTP APIs and web apps running on AKS:

Track request rate, latency, failures, dependencies (DB, Redis, etc.).

Correlate App Insights traces with pod name / container ID from logs.

Metrics & alerts

Create Azure Monitor alerts on:

Cluster level: node CPU/memory > threshold, disk pressure, ‚Äúnode not ready‚Äù.

Workload level: pod restarts, pod not ready, HPA not scaling, 5xx rate from ingress.

App level: request failure % and p95 latency from App Insights.

Alerts go to email / Teams channel via action groups.

Logging

All AKS logs ‚Üí Log Analytics Workspace (kube events, container logs, controller logs).

Use Kusto queries to filter for:

specific namespaces

error messages or correlation IDs

repeated CrashLoopBackOff / ImagePullBackOff issues

2Ô∏è‚É£ Tools I use for troubleshooting

Azure side

Azure Portal

AKS resource health, node pool status, upgrade status.

Metrics Explorer for quick checks (CPU, memory, node count, HPA metrics).

Azure Monitor / Log Analytics

Kusto queries on:

KubePodInventory, KubeNodeInventory, KubeEvents, ContainerLog

Dashboards to see:

Top pods by CPU/memory

Pods frequently restarting

Error logs aggregated by namespace/service

Network tools

Network Watcher / Connection Monitor for connectivity checks.

NSG / UDR review when traffic isn‚Äôt flowing as expected.

Kubernetes side

kubectl get/describe for:

nodes, pods, deployments, replicasets, services, ingress, events

kubectl logs and kubectl logs -f for live debugging.

kubectl top nodes/pods once metrics-server is enabled.

Occasionally kubectl exec into a pod to:

run DNS tests (nslookup, dig)

curl internal service endpoints

Helm (in your case): helm history, helm status, helm rollback for deployment issues.

3Ô∏è‚É£ Types of issues I‚Äôve actually resolved

I‚Äôd pick 2‚Äì3 examples in the interview; here are ones that match your experience.

‚úÖ Example 1: High CPU & pod evictions on AKS

Symptoms

Alerts from Azure Monitor: node CPU > 80‚Äì90% and pod evictions.

Users reported slowness and occasional 5xx from the application.

Steps I took

Checked Azure Monitor ‚Üí Container Insights:

Confirmed CPU saturation on specific nodes.

Saw that pods from a particular namespace were the top CPU consumers.

Ran:

kubectl top nodes
kubectl top pods -n <namespace>
kubectl describe pod <name> -n <namespace>


Found pods getting evicted due to resource pressure.

Noticed many containers had no proper requests/limits or very low requests.

Checked HPA configuration:

HPA was configured on CPU but the pods didn‚Äôt have realistic requests, so scaling was not behaving correctly.

Fix:

Updated resource requests/limits based on actual usage from metrics.

Tuned HPA threshold and min/max replicas.

For long-term, scaled the node pool and separated batch workloads into a different pool.

Result

CPU pressure reduced.

Evictions stopped.

Application response time became stable.

‚úÖ Example 2: ImagePullBackOff from ACR

Symptoms

New AKS deployment failed with ImagePullBackOff.

kubectl describe pod showed errors pulling image from Azure Container Registry.

Steps I took

Checked pod events:

kubectl describe pod <pod> -n <namespace>


Error clearly said unauthorized or failed to pull image.

Validated AKS ‚Üí ACR authentication:

Confirmed if the cluster was using Managed Identity or service principal.

Checked role assignments on ACR.

Fix:

Gave the AKS managed identity AcrPull role on ACR.

Verified image name / tag in Deployment YAML (sometimes tag mismatch was the root cause).

Did a rollout restart:

kubectl rollout restart deployment <deployment> -n <namespace>


Result

Pods started successfully once permissions and tag were corrected.

‚úÖ Example 3: DNS resolution / service connectivity issues

Symptoms

Application intermittent failures; logs showing DNS or connection timeouts.

Some services could not reach other internal services.

Steps I took

Checked CoreDNS:

kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl logs -n kube-system <coredns-pod>


Verified pods were running and not throttled.

Used a debug pod:

kubectl run dnsutils --image=busybox:1.28 -it --restart=Never -- nslookup <service>.<namespace>.svc.cluster.local


Tested DNS resolution and connectivity.

Reviewed:

Network Policies, NSGs, and UDRs for blocked traffic.

Any recent changes in VNET / DNS server configuration (if using custom DNS).

Fix:

Adjusted DNS settings / CoreDNS resource limits if it was being throttled.

Fixed incorrect service names or ports in application config.

Corrected any restrictive NetworkPolicies blocking pod-to-pod communication.

Result

DNS lookups became stable, and the intermittent failures stopped.

4Ô∏è‚É£ How I‚Äôd summarize in an interview

You can close with something like:

‚ÄúOverall, I treat AKS monitoring as a combination of Azure Monitor + App Insights + Log Analytics on the Azure side, and kubectl/Helm and proper resource 
configuration on the Kubernetes side. I‚Äôve handled issues like high CPU and pod evictions, ImagePullBackOff from ACR, DNS and connectivity issues inside the 
cluster, and misconfigured probes causing CrashLoopBackOff. I always start from metrics and logs, narrow down whether the problem is at cluster, node, pod, or 
app level, and then fix configuration or capacity accordingly.‚Äù

Q) Steps to take if kubelet goes down on a Kubernetes worker node
1Ô∏è‚É£ Check the node status
First verify the node state:

kubectl get nodes


If kubelet is down, the node will show:

NotReady

Unknown

2Ô∏è‚É£ SSH into the node

Log in to the node:

ssh <node-ip-or-name>

3Ô∏è‚É£ Check kubelet service status

On the node:

sudo systemctl status kubelet


You may see:

inactive

failed

crash loop

not running

4Ô∏è‚É£ Restart the kubelet service
sudo systemctl restart kubelet


Then verify:

sudo systemctl status kubelet

5Ô∏è‚É£ Check kubelet logs to find the root cause
sudo journalctl -u kubelet -f


Look for:

certificate errors

config file issues

CNI/network plugin errors

container runtime failures (containerd/Docker)

6Ô∏è‚É£ Check container runtime (containerd or Docker)

Because kubelet depends on it:

containerd:

sudo systemctl status containerd
sudo systemctl restart containerd


Docker:

sudo systemctl status docker
sudo systemctl restart docker


If the runtime is down ‚Üí kubelet won‚Äôt work.

7Ô∏è‚É£ Check disk pressure / memory pressure

Kubelet may stop if node resources are exhausted.

Check disk:

df -h


Check memory:

free -m


If disk is full ‚Üí clean logs:

sudo rm -rf /var/log/*-???????? /var/log/*.gz
sudo journalctl --vacuum-size=200M

8Ô∏è‚É£ Check kubelet config files

Configuration issues cause kubelet to fail.

Check:

/var/lib/kubelet/config.yaml
/etc/kubernetes/kubelet.conf
/etc/kubernetes/manifests/


Validate syntax if modified.

9Ô∏è‚É£ Check CNI plugin status

CNI issues also stop kubelet.

Check CNI directory:

ls -l /etc/cni/net.d/


Check CNI logs:

journalctl -u kubelet | grep CNI

üîü If unable to recover ‚Äî drain the node

If kubelet cannot be fixed quickly, move workloads safely:

On the master:

kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data


This reschedules workloads to healthy nodes.

1Ô∏è‚É£1Ô∏è‚É£ Remove and rejoin the node (if needed)

If kubelet or the node is corrupted:

Remove the node:

kubectl delete node <node-name>


Rejoin using your cluster‚Äôs join command:

kubeadm join ...

‚≠ê Interview-ready summary

If kubelet goes down, the node becomes NotReady, but pods continue running unmanaged.
I first check kubelet service, restart it, and review logs to identify root cause (runtime issues, disk pressure, CNI issues, certificate errors).
If the node can‚Äôt be recovered quickly, I drain it to reschedule workloads safely, then fix or rejoin the node.
